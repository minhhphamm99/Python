# =============================================================================
# 1. construct the graph of Y on X
# 2. Decide the set of lines potentially matching the data
# 3. conduct the test. Make the correspndednt regressions, make a decision which regression should be selected
# 4.From the selected model, analyse what coefficients are not 0
# 5. maka a conclusion, draw the data+the regression line
# =============================================================================

import random as rd
rd.seed(123)
Y=[]
X=[]
for i in range(0,300):
    X.append(rd.uniform(-3, 3))
    Y.append(X[i]**4-X[i]**3-5*X[i]**2+X[i]+5+rd.normalvariate(0,5))
  
import pandas as pd    
OriginalData=pd.DataFrame({"Y":Y, "X":X})

import matplotlib.pyplot as plt
plt.scatter(OriginalData.X,OriginalData.Y)
plt.show()

import numpy as np
from scipy import stats
#make a starting point, iterate the process for automatic generation of M1,2,3,4... dataframe
BIC=[]
BIC_X=[]

Ydf=OriginalData[OriginalData.columns[0]]
y=Ydf.values
Xdf=pd.DataFrame({"C":np.ones(len(OriginalData.X)), "X":OriginalData.X})
for i in range(2,10):
    xp=[]
    for j in range(0, len(Xdf.X)):
        xp.append(Xdf.X[j]**i)
    ColumnName='X'+str(i)
    Xdf[ColumnName]=xp    
    #on each step conduct calculation of BIC value
    x=Xdf.values
    B=np.matmul(np.matmul(np.linalg.inv(np.matmul(x.transpose(),x)),x.transpose()),y) #coefficients
    e=np.subtract(np.matmul(x,B),y)
    SSE=np.matmul(e.transpose(),e) # Sum of squared errors for BIC
    BIC.append(len(y)*np.log(SSE)+i*np.log(len(y)))
    BIC_X.append(i)

#check BIC plot
plt.plot(BIC_X,BIC)
plt.show()
# check the graph, find the minimal value
# it is the 4th item, correspondent to the 4th power of equation. Limit the dataframe
Xdf=Xdf[Xdf.columns[:5]]
#calculate for regression and all the rest parameeters  for the selected model

x=Xdf.values

B=np.matmul(np.matmul(np.linalg.inv(np.matmul(x.transpose(),x)),x.transpose()),y)

e=np.subtract(np.matmul(x,B),y)
MSE=np.matmul(e.transpose(),e)/(len(e)-len(Xdf.columns))
s=np.sqrt(MSE)
var_b=MSE*(np.linalg.inv(np.matmul(x.transpose(),x)).diagonal())
sd_b = np.sqrt(var_b)
ts_b=B/sd_b
p_values =np.round([2*(1-stats.t.cdf(np.abs(i),(len(y)-len(Xdf.columns)))) for i in ts_b],3) # check if there are any p_values higher that wanted threshold 

SSE=np.matmul(e.transpose(),e)
SST=np.sum((y-np.mean(y))**2)

Rsq=1-SSE/SST
# =============================================================================
# plt.plot(Xdf.X.values,np.matmul(x,B),color='r')
# =============================================================================
plt.show()

#Sorting dataframe to make a connected curve
temp=pd.DataFrame(Xdf.X.values,columns=['Value'])
temp['Target']=np.matmul(x,B)
#Sort the dataframe
temp=temp.sort_values(by=['Value'])
#plot the regression line
plt.scatter(Xdf.X.values,y,color='b')
plt.plot(temp['Value'],temp['Target'],c='r',linewidth=4)

#prediction interval
#confidence interval
ci=1.96*np.std(np.random.choice(y,100))/np.sqrt(100) 
Lowerci=temp['Value']-ci
Upperci=temp['Value']+ci  
plt.plot(Lowerci,temp['Target'],c='y',linewidth=4)       